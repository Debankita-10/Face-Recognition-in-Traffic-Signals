# -*- coding: utf-8 -*-
"""Face_Verification_in_traffic_signal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1koN-sSZMM-VyN5pzUETQwLISnCOuPIri
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import os
import random
import glob
import cv2
from sklearn.model_selection import train_test_split
from zipfile import ZipFile

# --- CONFIGURATION ---
IMG_SIZE = 128
EMBEDDING_DIM = 64
MARGIN = 1.0 # Margin for Contrastive Loss
EPOCHS = 20
BATCH_SIZE = 32

# NOTE: Adjust these paths to match your uploaded zip file names
REFERENCE_ZIP_NAME = "/content/short_references_final.zip" # Assuming 'difference' is your reference/clean set
DISTORTION_ZIP_NAME ="/content/Short_distortion_final.zip"
OUTPUT_DIR = './data_extracted'

# Create output directory if it doesn't exist
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

print("Starting file extraction...")

# Unzip Reference/Clean Images
try:
    with ZipFile(REFERENCE_ZIP_NAME, 'r') as zip_ref:
        zip_ref.extractall(OUTPUT_DIR + '/ref')
    print(f"‚úÖ Successfully extracted {REFERENCE_ZIP_NAME} to {OUTPUT_DIR}/ref")
except FileNotFoundError:
    print(f"‚ö†Ô∏è ERROR: {REFERENCE_ZIP_NAME} not found. Please upload it to your Colab environment.")

# Unzip Distorted Images
try:
    with ZipFile(DISTORTION_ZIP_NAME, 'r') as zip_ref:
        zip_ref.extractall(OUTPUT_DIR + '/distorted')
    print(f"‚úÖ Successfully extracted {DISTORTION_ZIP_NAME} to {OUTPUT_DIR}/distorted")
except FileNotFoundError:
    print(f"‚ö†Ô∏è ERROR: {DISTORTION_ZIP_NAME} not found. Please upload it to your Colab environment.")

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import os
import random
import glob
import cv2
import hashlib
import io
import base64
from PIL import Image as PILImage
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js

# --- 1. CONFIGURATION AND CONSTANTS ---
IMG_SIZE = 128
EMBEDDING_DIM = 64
MARGIN = 1.0
EPOCHS = 10
STEPS_PER_EPOCH = 32
BATCH_SIZE = 32
VERIFICATION_THRESHOLD = 0.6
MODEL_FILE_CLEAN = "face_embedding_model_CLEAN.h5"

OUTPUT_DIR = './data_extracted'
NEW_BASE_DIR = './data_refined'
CLEAN_REF_DIR = os.path.join(NEW_BASE_DIR, 'CLEAN_REF')
DISTORTED_IMG_DIR = os.path.join(NEW_BASE_DIR, 'DISTORTED_IMG')

# --- 2. CORE HELPER FUNCTIONS ---
def person_id_from_filename(filename):
    """Handles both 'ID.jpg' and 'ID__distortion.jpg' patterns."""
    base_name = os.path.basename(filename)
    if '__' in base_name:
        return base_name.split('__')[0]
    elif '.' in base_name:
        return base_name.split('.')[0]
    return "UNKNOWN"

def get_file_hash(filepath):
    """Calculates SHA256 hash of a file for duplicate detection."""
    hasher = hashlib.sha256()
    with open(filepath, 'rb') as file:
        while True:
            chunk = file.read(4096)
            if not chunk:
                break
            hasher.update(chunk)
    return hasher.hexdigest()

def load_and_preprocess_image(path):
    """Loads, resizes, and normalizes a single image from path."""
    img = cv2.imread(path)
    if img is None:
        return None
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    img = img.astype('float32') / 255.0
    return img

# --- 3. MODEL ARCHITECTURE AND LOSS ---
def build_feature_extractor(input_shape):
    input_tensor = keras.Input(shape=input_shape)
    x = layers.Conv2D(32, (3, 3), activation="relu")(input_tensor)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(64, (3, 3), activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(128, (3, 3), activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation="relu")(x)
    output_embedding = layers.Dense(EMBEDDING_DIM)(x)
    return keras.Model(input_tensor, output_embedding, name="feature_extractor")

def build_siamese_model(input_shape):
    feature_extractor = build_feature_extractor(input_shape)
    input_A = keras.Input(shape=input_shape, name="input_ref")
    input_B = keras.Input(shape=input_shape, name="input_distorted")
    embedding_A = feature_extractor(input_A)
    embedding_B = feature_extractor(input_B)
    squared_distance = layers.Lambda(
        lambda x: tf.reduce_sum(tf.square(x[0] - x[1]), axis=-1, keepdims=True)
    )([embedding_A, embedding_B])
    distance = layers.Lambda(lambda x: tf.sqrt(tf.maximum(x, 1e-6)), name="euclidean_distance")(squared_distance)
    return keras.Model(inputs=[input_A, input_B], outputs=distance, name="Siamese_Verification_Model")

def contrastive_loss(y_true, y_pred, margin=MARGIN):
    square_pred = tf.square(y_pred)
    margin_square = tf.square(tf.maximum(margin - y_pred, 0))
    loss = tf.cast(y_true, tf.float32) * square_pred + (1 - tf.cast(y_true, tf.float32)) * margin_square
    return tf.reduce_mean(loss)

def siamese_pair_generator(image_paths_by_person, person_ids, batch_size):
    while True:
        ref_images = []
        dist_images = []
        labels = []
        for _ in range(batch_size):
            person_id = random.choice(person_ids)
            paths = image_paths_by_person.get(person_id, [])
            if len(paths) < 2:
                continue

            # 50% Positive, 50% Negative
            if random.random() < 0.5:
                # Positive Pair (Same Person)
                img_path1, img_path2 = random.sample(paths, 2)
                label = 1.0
            else:
                # Negative Pair (Different People)
                p1_id, p2_id = random.sample(person_ids, 2)
                img_path1 = random.choice(image_paths_by_person.get(p1_id, []))
                img_path2 = random.choice(image_paths_by_person.get(p2_id, []))
                label = 0.0

            img1 = load_and_preprocess_image(img_path1)
            img2 = load_and_preprocess_image(img_path2)

            if img1 is not None and img2 is not None:
                ref_images.append(img1)
                dist_images.append(img2)
                labels.append(label)

        if ref_images:
            yield ((np.array(ref_images), np.array(dist_images)), np.array(labels))

# --- 4. REAL-TIME VERIFICATION LOGIC ---
def identify_face_from_frame(face_array, embedding_model, gallery, threshold):
    """Preprocesses a cropped face array and identifies the person."""
    processed_face = cv2.resize(face_array, (IMG_SIZE, IMG_SIZE))
    processed_face = processed_face.astype('float32') / 255.0
    query_emb = embedding_model.predict(np.expand_dims(processed_face, axis=0), verbose=0)[0]

    min_distance = float('inf')
    best_match_id = "UNKNOWN"

    for person_id, emb in gallery.items():
        distance = np.linalg.norm(query_emb - emb)
        if distance < min_distance:
            min_distance = distance
            best_match_id = person_id

    if min_distance < threshold:
        return best_match_id, min_distance, "MATCH"
    else:
        return "Unknown Person", min_distance, "REJECT"

def get_webcam_frame_colab(quality=0.8):
    """Captures a single frame from the browser's webcam."""
    js = Javascript('''
        async function capture() {
            const div = document.createElement('div');
            const video = document.createElement('video');
            video.style.display = 'block';
            const stream = await navigator.mediaDevices.getUserMedia({video: true});
            document.body.appendChild(div);
            div.appendChild(video);
            video.srcObject = stream;
            await video.play();

            await new Promise((resolve) => video.onplaying = resolve);

            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);

            stream.getVideoTracks()[0].stop();
            document.body.removeChild(div);

            return canvas.toDataURL('image/jpeg', %s);
        }
        capture();
    ''' % quality)

    data = eval_js(js.data)
    binary = base64.b64decode(data.split(',')[1])
    img = PILImage.open(io.BytesIO(binary))
    frame = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
    return frame

def detect_and_crop_face(frame):
    """Uses Haar Cascade to find and crop the largest face."""
    FACE_CASCADE = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = FACE_CASCADE.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    if len(faces) == 0:
        return None, None

    (x, y, w, h) = max(faces, key=lambda rect: rect[2] * rect[3])
    buffer = int(0.1 * w)
    x = max(0, x - buffer)
    y = max(0, y - buffer)
    w = w + 2 * buffer
    h = h + 2 * buffer

    cropped_face = frame[y:y + h, x:x + w]
    return cropped_face, (x, y, w, h)

# ---------------------------------------------------------------------
# --- 5. FINAL WORKFLOW EXECUTION ---
# ---------------------------------------------------------------------
if __name__ == '__main__':
    import shutil
    input_shape = (IMG_SIZE, IMG_SIZE, 3)

    # Ensure Haar Cascade is downloaded for later use
    !wget -nc https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml

    # --- Part 1: DATA PREPARATION AND INDEXING ---
    reference_images_by_person = {}
    distorted_images_by_person = {}

    for root, _, files in os.walk(OUTPUT_DIR):
        for f in files:
            path = os.path.join(root, f)
            person_id = person_id_from_filename(f)
            if person_id != "UNKNOWN":
                if 'ref' in root.lower() or 'difference' in root.lower():
                    reference_images_by_person.setdefault(person_id, []).append(path)
                elif 'distorted' in root.lower():
                    distorted_images_by_person.setdefault(person_id, []).append(path)

    PERSON_IDS = list(set(reference_images_by_person.keys()) & set(distorted_images_by_person.keys()))

    merged_image_paths = {k: reference_images_by_person.get(k, []) + distorted_images_by_person.get(k, [])
                          for k in PERSON_IDS}

    # --- Part 2: TRAIN OR LOAD MODEL ---
    if os.path.exists(MODEL_FILE_CLEAN):
        print("\n--- Model Found: Loading CLEAN Model ---")
        final_extractor_clean = keras.models.load_model(MODEL_FILE_CLEAN, compile=False)
    else:
        print("\n--- Model NOT Found: STARTING FULL RE-TRAINING ---")
        siamese_model = build_siamese_model(input_shape)
        siamese_model.compile(optimizer=keras.optimizers.Adam(1e-4), loss=contrastive_loss)
        train_generator = siamese_pair_generator(merged_image_paths, PERSON_IDS, BATCH_SIZE)

        siamese_model.fit(train_generator, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, verbose=1)

        # ‚úÖ Save the trained feature extractor
        feature_extractor = siamese_model.get_layer("feature_extractor")
        feature_extractor.save("face_embedding_model_CLEAN.h5", include_optimizer=False)

        # Verify full model size
        print("‚úÖ Saved Model Size (MB):", os.path.getsize("face_embedding_model_CLEAN.h5") / 1024 / 1024)

        # Download model to your machine
        from google.colab import files
        files.download("face_embedding_model_CLEAN.h5")

        final_extractor_clean = feature_extractor
        print("‚úÖ Re-training complete.")

    # --- Part 3: BUILD GALLERY ---
    gallery_embeddings = {}
    print("\n--- Building Final Gallery Embeddings ---")

    for person_id in PERSON_IDS:
        ref_paths = reference_images_by_person.get(person_id)
        if not ref_paths:
            continue

        ref_path = ref_paths[0]
        ref_img = load_and_preprocess_image(ref_path)
        ref_emb = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
        gallery_embeddings[person_id] = ref_emb

    print(f"‚úÖ Gallery created for {len(gallery_embeddings)} people.")

# ‚úÖ Re-save and verify complete model
final_extractor_clean.save("face_embedding_model_CLEAN.h5", include_optimizer=False)

import os
print("Model size (MB):", os.path.getsize("face_embedding_model_CLEAN.h5") / 1024 / 1024)

from google.colab import files
files.download("face_embedding_model_CLEAN.h5")

final_extractor_clean.summary()

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # Check for all-black image data
    if np.all(ref_img == 0):
      print("WARNING: Reference image array is all zeros. Check load_and_preprocess_image for file loading error.")
    # You could try reloading the images without the normalization to see the raw data:
    # raw_img = cv2.imread(ref_path)

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import cv2
import numpy as np
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
import base64
import io
from PIL import Image as PILImage

# --- JAVASCRIPT/COLAB CAMERA ACCESS CODE ---

def get_webcam_frame_colab(quality=0.8):
    """
    Captures a single frame from the browser's webcam and returns it as an OpenCV BGR NumPy array.
    """
    js = Javascript('''
        async function capture() {
            // Setup elements
            const div = document.createElement('div');
            const video = document.createElement('video');
            video.style.display = 'block';
            const stream = await navigator.mediaDevices.getUserMedia({video: true});

            document.body.appendChild(div);
            div.appendChild(video);
            video.srcObject = stream;

            // Wait for video to load
            await new Promise((resolve) => video.onplaying = resolve);

            // Capture the frame
            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);

            // Stop the stream and remove elements
            stream.getVideoTracks()[0].stop();
            document.body.removeChild(div);

            return canvas.toDataURL('image/jpeg', %s);
        }
        capture();
    ''' % quality)

    # Execute JavaScript and get image data
    data = eval_js(js.data)

    # Decode image data from base64 to OpenCV format
    binary = base64.b64decode(data.split(',')[1])
    img = PILImage.open(io.BytesIO(binary))

    # Convert PIL Image (RGB) to OpenCV NumPy array (BGR)
    frame = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
    return frame

# --- END JAVASCRIPT/COLAB CAMERA ACCESS CODE ---

# --- FACE DETECTION PLACEHOLDER ---
# IMPORTANT: You must replace this with a real face detection library
# (e.g., MTCNN, or a simple Haar Cascade from cv2) to get accurate results.
def detect_and_crop_face(frame):
    """
    MOCK DETECTION: Tries to simulate where a face detection would be.
    For simplicity, we'll assume the face is in the center for this final output,
    but for a real app, this must be a robust detector.
    """
    # Using a simple fixed crop for demonstration (will fail if face is off-center)
    h, w, _ = frame.shape
    crop_size = min(h, w) // 2

    # Define bounding box at the center
    x = w // 2 - crop_size // 2
    y = h // 2 - crop_size // 2
    w = crop_size
    h = crop_size

    cropped_face = frame[y:y+h, x:x+w]
    return cropped_face, (x, y, w, h)

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, and the image path dictionaries
# are assumed to be loaded and correct from previous steps.

# --- 1. Random Selection and Path Retrieval ---
if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select a random person ID for the test
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get the distorted query image path (Positive)
    # We use the first distorted image for this person
    query_path = distorted_images_by_person[TEST_ID][0]

    # --- 2. Load Images and Calculate Distance ---

    # Load the images using your existing helper function
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (REJECTED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'Verification Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f}', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to make room for suptitle
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Randomly selected ID: {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (FINAL FIXED VERSION)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    try:
        # Clear previous TensorFlow models to avoid KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run gender analysis safely
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6

# --- 1. Random Selection of TWO DIFFERENT People ---

if len(PERSON_IDS) < 2:
    print("Error: Need at least two different person IDs to run a True Negative test.")
else:
    # Select two unique IDs
    anchor_id, query_id = random.sample(PERSON_IDS, 2)

    # --- 2. Get Image Paths ---
    # Anchor: Clean Reference Image of Person A
    ref_path = reference_images_by_person[anchor_id][0]

    # Query: Distorted Image of Person B (The False Match)
    # We take the first available distorted image of the second person.
    query_path = distorted_images_by_person[query_id][0]

    # --- 3. Load Images and Calculate Distance ---

    # Load the images
    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings (Requires final_extractor_clean to be loaded)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_negative = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict (We expect REJECTED)
    verdict = "Different Person (REJECTED)" if dist_negative >= VERIFICATION_THRESHOLD else "Same Person (FALSE MATCH!)"
    verdict_color = 'red' if dist_negative >= VERIFICATION_THRESHOLD else 'blue'

    # --- 4. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'True Negative Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference (Anchor)
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Anchor): {anchor_id}', fontsize=12)
    axes[0].set_xlabel(f'ID: {anchor_id}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query (Negative Pair)
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted): {query_id}', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_negative:.4f} (Threshold: {VERIFICATION_THRESHOLD})', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9])
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Comparison: {anchor_id} (Reference) vs. {query_id} (Query)")
    print(f"Calculated Distance: {dist_negative:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (Runs after verification)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    """Detect gender from an image using DeepFace (safe + robust)."""
    try:
        # Prevent KerasTensor graph conflicts (important if FaceNet or Keras used before)
        tf.keras.backend.clear_session()

        # Run DeepFace gender analysis
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )

        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# Detect gender for both reference and query images
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference ({anchor_id})\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query ({query_id})\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

# If both genders differ, print that info too
if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender != query_gender:
        print(f"‚ö° GENDER MISMATCH DETECTED: {ref_gender.upper()} vs {query_gender.upper()}")
    else:
        print(f"‚úÖ Both images appear to be of the same gender: {ref_gender.capitalize()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6
# NOTE: final_extractor_clean, gallery_embeddings, etc., are assumed to be loaded.

# --- 1. Random Selection for True Positive Test ---

if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select ONE random person ID (e.g., MND-2)
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get a random distorted image path for the same person (Query)
    # This proves it works with any one of the many distortions
    query_paths_list = distorted_images_by_person[TEST_ID]
    query_path = random.choice(query_paths_list)

    # --- 2. Load Images and Calculate Distance ---

    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (FALSE NEGATIVE!)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'True Positive Result: {verdict}', fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Anchor (Clean)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query (Shows the specific distortion used)
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Distorted: {os.path.basename(query_path).split("__")[-1]})', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f} (Threshold: {VERIFICATION_THRESHOLD})', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9])
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Comparison: Reference ID {TEST_ID} vs. Distorted ID {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (Runs after True Positive verification)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2
import tensorflow as tf
import os
import matplotlib.pyplot as plt

def detect_gender(image_path):
    """Detect gender from an image using DeepFace (safe + accurate)."""
    try:
        # Clear previous TensorFlow graph to prevent KerasTensor conflicts
        tf.keras.backend.clear_session()

        # Run DeepFace gender analysis
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )

        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# --- Detect gender for both images ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display the images with detected genders ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference (Anchor)\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query (Distorted)\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

# ‚úÖ Optional gender comparison
if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender != query_gender:
        print(f"‚ö° GENDER MISMATCH DETECTED: {ref_gender.upper()} vs {query_gender.upper()}")
    else:
        print(f"‚úÖ Both images appear to be of the same gender: {ref_gender.capitalize()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6

# --- 1. Random Selection for True Positive Test ---

if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select ONE random person ID (The correct identity for the test)
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path (Anchor)
    ref_path = reference_images_by_person[TEST_ID][0]

    # Get a random distorted image path for the same person (Query)
    query_paths_list = distorted_images_by_person[TEST_ID]
    query_path = random.choice(query_paths_list)

    # --- 2. Load Images and Calculate ALL Distances (Identification) ---

    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embedding for the QUERY image
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    min_dist_match = float('inf')  # Distance to the correct person's reference
    min_dist_wrong = float('inf')  # Distance to the CLOSEST INCORRECT person's reference
    closest_wrong_id = "N/A"

    # Compare Query against EVERYONE in the gallery
    for person_id, emb_ref in gallery_embeddings.items():
        distance = np.linalg.norm(emb_ref - emb_query)

        if person_id == TEST_ID:
            min_dist_match = distance # Found the distance to the correct match
        elif distance < min_dist_wrong:
            min_dist_wrong = distance # Found the distance to the closest wrong person
            closest_wrong_id = person_id

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
    verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    fig.suptitle(f'FULL IDENTIFICATION TEST: Result for {TEST_ID} ({verdict})',
                 fontsize=14, color=verdict_color, fontweight='bold')

    # Image 1: Correct Match Comparison
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. CORRECT REFERENCE: {TEST_ID}', fontsize=12)
    axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query Image (The Input)
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. INPUT QUERY (Distortion)', fontsize=12)
    axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9])
    plt.show()

    print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
    print(f"Query Image is: {os.path.basename(query_path)}")
    print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
    print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
    print(f"The model successfully separated D_match from D_wrong.")

# =========================================================
# üß† GENDER DETECTION SECTION (for Full Identification Test)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace safely."""
    try:
        tf.keras.backend.clear_session()  # Prevent model conflicts
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# --- Detect gender for both images ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display results visually ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference: {TEST_ID}\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

# ‚úÖ Optional gender consistency check
if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender != query_gender:
        print(f"‚ö° GENDER MISMATCH DETECTED: {ref_gender.upper()} vs {query_gender.upper()}")
    else:
        print(f"‚úÖ Both images appear to be of the same gender: {ref_gender.capitalize()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- Configuration is consistent ---

if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select ONE random person ID (The correct identity for the test)
    TEST_ID = random.choice(PERSON_IDS)

    # --- Strict Path Selection ---
    # 1. Anchor: Use the FIRST file in the CLEAN list (Assumption: This is the definitive clean image)
    ref_path = reference_images_by_person[TEST_ID][0]

    # 2. Query: Randomly select a DISTORTED image, but only if there is more than one.
    query_paths_list = distorted_images_by_person[TEST_ID]

    if len(query_paths_list) > 1:
        # If there are multiple distorted images, pick one randomly that is NOT the first.
        query_path = random.choice(query_paths_list[1:])
    else:
        # Fallback to the only distorted image available
        query_path = query_paths_list[0]


    # --- 2. Load Images and Calculate ALL Distances (Identification) ---

    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    min_dist_match = float('inf')
    min_dist_wrong = float('inf')
    closest_wrong_id = "N/A"

    # Compare Query against EVERYONE in the gallery
    for person_id, emb_ref in gallery_embeddings.items():
        distance = np.linalg.norm(emb_ref - emb_query)

        if person_id == TEST_ID:
            min_dist_match = distance # Found the distance to the correct match
        elif distance < min_dist_wrong:
            min_dist_wrong = distance # Found the distance to the closest wrong person
            closest_wrong_id = person_id

    # Determine the final verdict
    VERIFICATION_THRESHOLD = 0.6
    verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
    verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    fig.suptitle(f'FULL IDENTIFICATION TEST: Result for {TEST_ID} ({verdict})',
                 fontsize=14, color=verdict_color, fontweight='bold')

    # Image 1: Correct Match Comparison
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. CORRECT REFERENCE: {TEST_ID}', fontsize=12)
    axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query Image (The Input)
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. INPUT QUERY (Distortion)', fontsize=12)
    axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9])
    plt.show()

    print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
    print(f"Query Image Path used: {os.path.basename(query_path)}")
    print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
    print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")

# =========================================================
# üß† GENDER DETECTION SECTION (for Full Identification Test)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace safely."""
    try:
        tf.keras.backend.clear_session()  # Prevent model conflicts
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# --- Detect gender for both images ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display results visually ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference: {TEST_ID}\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

# ‚úÖ Optional gender consistency check
if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender != query_gender:
        print(f"‚ö° GENDER MISMATCH DETECTED: {ref_gender.upper()} vs {query_gender.upper()}")
    else:
        print(f"‚úÖ Both images appear to be of the same gender: {ref_gender.capitalize()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6

if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select ONE random person ID (The correct identity for the test)
    TEST_ID = random.choice(PERSON_IDS)

    # 1. Anchor: Use the clean reference image path
    ref_path = reference_images_by_person[TEST_ID][0]

    # 2. Query: Randomly select a DISTORTED image for the same person
    query_paths_list = distorted_images_by_person[TEST_ID]
    query_path = random.choice(query_paths_list)

    # --- 2. Load Images and Calculate ALL Distances (Identification) ---

    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embedding for the QUERY image
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    min_dist_match = float('inf')  # Distance to the correct person's reference
    min_dist_wrong = float('inf')  # Distance to the CLOSEST INCORRECT person's reference
    closest_wrong_id = "N/A"

    # Compare Query against EVERYONE in the gallery
    for person_id, emb_ref in gallery_embeddings.items():
        distance = np.linalg.norm(emb_ref - emb_query)

        if person_id == TEST_ID:
            min_dist_match = distance # Found the distance to the correct match
        elif distance < min_dist_wrong:
            min_dist_wrong = distance # Found the distance to the closest wrong person
            closest_wrong_id = person_id

    # Determine the final verdict
    VERIFICATION_THRESHOLD = 0.6
    verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
    verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

    # --- 3. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    fig.suptitle(f'FULL IDENTIFICATION TEST: Result for {TEST_ID} ({verdict})',
                 fontsize=14, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. CORRECT REFERENCE: {TEST_ID}', fontsize=12)
    axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query Image (The Input)
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. INPUT QUERY (Distortion)', fontsize=12)
    axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9])
    plt.show()

    print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
    print(f"Query Image Path used: {os.path.basename(query_path)}")
    print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
    print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
    print(f"Final Verdict: {verdict}")

# =========================================================
# üß† GENDER DETECTION SECTION (for Full Identification Test)
# =========================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace safely."""
    try:
        tf.keras.backend.clear_session()  # Prevent model conflicts
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence

# --- Detect gender for both images ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)

# --- Display results visually ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference: {TEST_ID}\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

# ‚úÖ Optional gender consistency check
if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender != query_gender:
        print(f"‚ö° GENDER MISMATCH DETECTED: {ref_gender.upper()} vs {query_gender.upper()}")
    else:
        print(f"‚úÖ Both images appear to be of the same gender: {ref_gender.capitalize()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION is consistent ---
VERIFICATION_THRESHOLD = 0.6

if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select ONE random person ID (The correct identity for the test)
    TEST_ID = random.choice(PERSON_IDS)

    # 1. Anchor: Use the clean reference image path (Index 0 is fine here)
    ref_path = reference_images_by_person[TEST_ID][0]

    # 2. Query: Force Selection of a DIFFERENT Distorted Image (Index 1 or later)
    query_paths_list = distorted_images_by_person[TEST_ID]

    if len(query_paths_list) > 1:
        # Use slicing [1:] to skip the first (potentially corrupted) file, then pick randomly.
        non_identical_distortions = query_paths_list[1:]
        query_path = random.choice(non_identical_distortions)
    else:
        # Fallback if only one file exists (meaning the test will be D=0.0000)
        query_path = query_paths_list[0]


    # --- 3. Load Images and Calculate ALL Distances (Identification) ---

    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embedding for the QUERY image
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    min_dist_match = float('inf')
    min_dist_wrong = float('inf')
    closest_wrong_id = "N/A"

    # Compare Query against EVERYONE in the gallery
    for person_id, emb_ref in gallery_embeddings.items():
        distance = np.linalg.norm(emb_ref - emb_query)

        if person_id == TEST_ID:
            min_dist_match = distance # Found the distance to the correct match
        elif distance < min_dist_wrong:
            min_dist_wrong = distance # Found the distance to the closest wrong person
            closest_wrong_id = person_id

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
    verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

    # --- 4. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    fig.suptitle(f'FORCED TRUE POSITIVE TEST: Result for {TEST_ID} ({verdict})',
                 fontsize=14, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. CORRECT REFERENCE: {TEST_ID}', fontsize=12)
    axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query Image (The Input)
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. INPUT QUERY (Distortion)', fontsize=12)
    axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9])
    plt.show()

    print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
    print(f"Query Image Path used: {os.path.basename(query_path)}")
    print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
    print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
    print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (for Forced True Positive Test)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace safely (handles errors gracefully)."""
    try:
        tf.keras.backend.clear_session()  # Avoid model conflicts
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # Ensures it works even if face not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Detect gender for Reference and Query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display Gender Results Visually ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference ({TEST_ID})\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query (Distorted)\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

# --- Print Summary in Console ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

# ‚úÖ Optional Gender Consistency Check
if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender != query_gender:
        print(f"‚ö° GENDER MISMATCH DETECTED: {ref_gender.upper()} vs {query_gender.upper()}")
    else:
        print(f"‚úÖ Both images appear to be of the same gender: {ref_gender.capitalize()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION is consistent ---
VERIFICATION_THRESHOLD = 0.6
IDENTICAL_DISTANCE_TOLERANCE = 0.001 # Treat any distance <= 0.001 as identical/corrupted

if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # 1. Select ONE random person ID (The correct identity for the test)
    TEST_ID = random.choice(PERSON_IDS)

    # Get the clean reference image path
    ref_path = reference_images_by_person[TEST_ID][0]
    ref_img = load_and_preprocess_image(ref_path)
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]

    # --- 2. DYNAMICALLY FILTER OUT IDENTICAL FILES ---

    clean_distorted_paths = []

    # Iterate through ALL distorted files for the chosen person
    for query_path in distorted_images_by_person[TEST_ID]:
        query_img = load_and_preprocess_image(query_path)
        if query_img is None: continue

        emb_query_check = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

        # Calculate distance between Reference and this specific Distortion
        distance_check = np.linalg.norm(emb_ref - emb_query_check)

        # Keep the path ONLY if the distance is greater than the tolerance (i.e., not identical)
        if distance_check > IDENTICAL_DISTANCE_TOLERANCE:
            clean_distorted_paths.append(query_path)

    # --- 3. EXECUTE TEST ON CLEANED QUERY LIST ---

    if not clean_distorted_paths:
        print(f"\n‚ùå FATAL DATA ERROR: All distorted images for {TEST_ID} are mathematically identical to the reference image. Cannot run True Positive test.")
    else:
        # Randomly select ONE genuinely non-identical distorted image
        query_path = random.choice(clean_distorted_paths)

        # Re-load the chosen query image (already checked above, but for final plotting)
        query_img = load_and_preprocess_image(query_path)
        emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

        # Recalculate D_match (should be > 0.001)
        min_dist_match = np.linalg.norm(emb_ref - emb_query)

        # --- Check Closest Wrong ID for comparison (Final Identification) ---
        min_dist_wrong = float('inf')
        closest_wrong_id = "N/A"

        for person_id, emb_ref_wrong in gallery_embeddings.items():
            if person_id != TEST_ID:
                distance = np.linalg.norm(emb_ref_wrong - emb_query)
                if distance < min_dist_wrong:
                    min_dist_wrong = distance
                    closest_wrong_id = person_id

        # Determine the final verdict
        verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FALSE NEGATIVE!)"
        verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

        # --- 4. Plotting the Final Output ---

        fig, axes = plt.subplots(1, 2, figsize=(12, 6))

        fig.suptitle(f'TRUE POSITIVE IDENTIFICATION TEST: Result for {TEST_ID} ({verdict})',
                     fontsize=14, color=verdict_color, fontweight='bold')

        # Image 1: Clean Reference
        axes[0].imshow(ref_img)
        axes[0].set_title(f'1. CORRECT REFERENCE: {TEST_ID}', fontsize=12)
        axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
        axes[0].axis('off')

        # Image 2: Distorted Query Image (The Input)
        axes[1].imshow(query_img)
        axes[1].set_title(f'2. INPUT QUERY (Distortion)', fontsize=12)
        axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
        axes[1].axis('off')

        plt.tight_layout(rect=[0, 0.03, 1, 0.9])
        plt.show()

        print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
        print(f"Query Image Path used: {os.path.basename(query_path)}")
        print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
        print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
        print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (for True Positive Identification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace safely (with fallback handling)."""
    try:
        tf.keras.backend.clear_session()  # Prevents model conflicts
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # Works even if face not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run gender detection for both reference and query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display Gender Results Visually ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference ({TEST_ID})\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query (Distorted)\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()

# --- Print Summary in Console ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

# ‚úÖ Optional Gender Consistency Check
if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender != query_gender:
        print(f"‚ö° GENDER MISMATCH DETECTED: {ref_gender.upper()} vs {query_gender.upper()}")
    else:
        print(f"‚úÖ Both images appear to be of the same gender: {ref_gender.capitalize()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- Configuration is consistent ---
VERIFICATION_THRESHOLD = 0.6

if len(PERSON_IDS) < 2:
    print("Error: Need at least two unique people for this comparison.")
else:
    # 1. Select ONE Anchor ID (The Reference) and ONE Negative ID (The Query)
    anchor_id, query_id = random.sample(PERSON_IDS, 2)

    # --- 2. Get Image Paths ---

    # ANCHOR (Reference): Clean image of Person A
    anchor_ref_path = reference_images_by_person[anchor_id][0]

    # QUERY (Distorted): Distorted image of Person B (The ultimate True Negative test)
    query_distorted_path = random.choice(distorted_images_by_person[query_id])

    # --- 3. Load Images and Generate Embeddings ---

    ref_img = load_and_preprocess_image(anchor_ref_path)
    query_img = load_and_preprocess_image(query_distorted_path)

    emb_anchor = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # --- 4. Calculate Distance ---
    dist_negative = np.linalg.norm(emb_anchor - emb_query) # Expected HIGH distance

    # Determine verdict
    verdict = "REJECTED (Correct Separation)" if dist_negative >= VERIFICATION_THRESHOLD else "FALSE MATCH (Failed Separation)"
    verdict_color = 'green' if dist_negative >= VERIFICATION_THRESHOLD else 'red'

    # --- 5. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))
    fig.suptitle(f'TRUE NEGATIVE TEST: {anchor_id} vs. {query_id}', fontsize=16, fontweight='bold')

    # Column 1: ANCHOR (Reference)
    axes[0].imshow(ref_img)
    axes[0].set_title(f'ANCHOR: {anchor_id} (Clean Reference)', fontsize=12)
    axes[0].axis('off')

    # Column 2: NEGATIVE QUERY (Distorted Different Person)
    axes[1].imshow(query_img)
    axes[1].set_title(f'QUERY: {query_id} (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_negative:.4f} ({verdict})', color=verdict_color)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9])
    plt.show()

    print(f"\n--- CONSOLE SUMMARY ---")
    print(f"Comparison: Reference ID {anchor_id} vs. Distorted ID {query_id}")
    print(f"Calculated Distance: {dist_negative:.4f}")
    print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (for True Negative Identification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace safely with error handling."""
    try:
        tf.keras.backend.clear_session()  # Avoids KerasTensor conflicts
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # Works even if face is not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Both Reference and Query ---
anchor_gender, anchor_conf = detect_gender(anchor_ref_path)
query_gender, query_conf = detect_gender(query_distorted_path)


# --- Display the Results Side-by-Side ---
ref_img_display = cv2.cvtColor(cv2.imread(anchor_ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_distorted_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (True Negative Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference: {anchor_id}\nGender: {anchor_gender.capitalize()} ({anchor_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query: {query_id}\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Detection Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(anchor_ref_path)}): {anchor_gender} ({anchor_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_distorted_path)}): {query_gender} ({query_conf:.1f}%)")

# ‚úÖ Gender Consistency Check
if anchor_gender != "Unknown" and query_gender != "Unknown":
    if anchor_gender != query_gender:
        print(f"‚úÖ Expected Gender Difference Detected: {anchor_gender.upper()} vs {query_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Both appear to be the same gender ({anchor_gender.capitalize()}) ‚Äî verify dataset labeling.")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- Configuration is consistent ---
VERIFICATION_THRESHOLD = 0.6

if len(PERSON_IDS) < 2:
    print("Error: Need at least two unique people for this comparison.")
else:
    # 1. Select ONE Anchor ID (The Reference) and ONE Negative ID (The Query)
    anchor_id, query_id = random.sample(PERSON_IDS, 2)

    # --- 2. Get Image Paths ---

    # ANCHOR (Reference): Clean image of Person A
    anchor_ref_path = reference_images_by_person[anchor_id][0]

    # QUERY (Distorted): Distorted image of Person B (The ultimate True Negative test)
    query_distorted_path = random.choice(distorted_images_by_person[query_id])

    # --- 3. Load Images and Generate Embeddings ---

    ref_img = load_and_preprocess_image(anchor_ref_path)
    query_img = load_and_preprocess_image(query_distorted_path)

    emb_anchor = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # --- 4. Calculate Distance ---
    dist_negative = np.linalg.norm(emb_anchor - emb_query) # Expected HIGH distance

    # Determine verdict
    verdict = "REJECTED (Correct Separation)" if dist_negative >= VERIFICATION_THRESHOLD else "FALSE MATCH (Failed Separation)"
    verdict_color = 'green' if dist_negative >= VERIFICATION_THRESHOLD else 'red'

    # --- 5. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))
    fig.suptitle(f'TRUE NEGATIVE TEST: {anchor_id} vs. {query_id}', fontsize=16, fontweight='bold')

    # Column 1: ANCHOR (Reference)
    axes[0].imshow(ref_img)
    axes[0].set_title(f'ANCHOR: {anchor_id} (Clean Reference)', fontsize=12)
    axes[0].axis('off')

    # Column 2: NEGATIVE QUERY (Distorted Different Person)
    axes[1].imshow(query_img)
    axes[1].set_title(f'QUERY: {query_id} (Distorted)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_negative:.4f} ({verdict})', color=verdict_color)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9])
    plt.show()

    print(f"\n--- CONSOLE SUMMARY ---")
    print(f"Comparison: Reference ID {anchor_id} vs. Distorted ID {query_id}")
    print(f"Calculated Distance: {dist_negative:.4f}")
    print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (for True Negative Identification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace safely with error handling."""
    try:
        tf.keras.backend.clear_session()  # Avoids KerasTensor conflicts
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # Works even if face is not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Both Reference and Query ---
anchor_gender, anchor_conf = detect_gender(anchor_ref_path)
query_gender, query_conf = detect_gender(query_distorted_path)


# --- Display the Results Side-by-Side ---
ref_img_display = cv2.cvtColor(cv2.imread(anchor_ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_distorted_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (True Negative Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference: {anchor_id}\nGender: {anchor_gender.capitalize()} ({anchor_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query: {query_id}\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Detection Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(anchor_ref_path)}): {anchor_gender} ({anchor_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_distorted_path)}): {query_gender} ({query_conf:.1f}%)")

# ‚úÖ Gender Consistency Check
if anchor_gender != "Unknown" and query_gender != "Unknown":
    if anchor_gender != query_gender:
        print(f"‚úÖ Expected Gender Difference Detected: {anchor_gender.upper()} vs {query_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Both appear to be the same gender ({anchor_gender.capitalize()}) ‚Äî verify dataset labeling.")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- CONFIGURATION ---
VERIFICATION_THRESHOLD = 0.6

if not PERSON_IDS:
    print("Error: PERSON_IDS list is empty. Cannot run test.")
else:
    # Select ONE random person ID (The correct identity for the test)
    TEST_ID = random.choice(PERSON_IDS)

    # 1. Anchor: Use the clean reference image path
    ref_path = reference_images_by_person[TEST_ID][0]

    # 2. Query: Randomly select ONE of the many available distorted images for the same person
    query_paths_list = distorted_images_by_person[TEST_ID]
    query_path = random.choice(query_paths_list) # Samples from ALL distortions

    # --- 3. Load Images and Calculate Distance ---

    ref_img = load_and_preprocess_image(ref_path)
    query_img = load_and_preprocess_image(query_path)

    # Generate embeddings
    emb_ref = final_extractor_clean.predict(np.expand_dims(ref_img, axis=0), verbose=0)[0]
    emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

    # Calculate the distance
    dist_positive = np.linalg.norm(emb_ref - emb_query)

    # Determine the final verdict
    verdict = "Same Person (VERIFIED)" if dist_positive < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
    verdict_color = 'green' if dist_positive < VERIFICATION_THRESHOLD else 'red'

    # --- 4. Plotting the Final Output ---

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Set the overall title with the model's final verification result
    fig.suptitle(f'True Positive Test: Result for {TEST_ID} ({verdict})',
                 fontsize=16, color=verdict_color, fontweight='bold')

    # Image 1: Clean Reference
    axes[0].imshow(ref_img)
    axes[0].set_title(f'1. Reference (Anchor)', fontsize=12)
    axes[0].set_xlabel(f'ID: {TEST_ID}', fontsize=12)
    axes[0].axis('off')

    # Image 2: Distorted Query (Shows the specific distortion used)
    axes[1].imshow(query_img)
    axes[1].set_title(f'2. Query (Random Distortion)', fontsize=12)
    axes[1].set_xlabel(f'Distance: {dist_positive:.4f} (Threshold: {VERIFICATION_THRESHOLD})', fontsize=12)
    axes[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.9])
    plt.show()

    print(f"\n--- CONSOLE OUTPUT ---")
    print(f"Comparison: Anchor ID {TEST_ID} vs. Distorted ID {TEST_ID}")
    print(f"Calculated Distance: {dist_positive:.4f}")
    print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (for True Positive Verification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace with built-in pretrained model."""
    try:
        tf.keras.backend.clear_session()  # Prevents TensorFlow session conflicts
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # Works even for partial/angled faces
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {image_path}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Both Reference and Query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display the Results Side-by-Side ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (True Positive Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference Image\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Detection Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference Image ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query Image ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

# ‚úÖ Gender Consistency Check
if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender == query_gender:
        print(f"‚úÖ Gender Match Verified: Both are {ref_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Gender Mismatch Detected: {ref_gender.upper()} vs {query_gender.upper()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- Configuration (Final Settings) ---
VERIFICATION_THRESHOLD = 0.6

# --- STEP 1: DEFINE YOUR TEST CASE ---
# 1. Set the ID of the person the clean image belongs to (The Anchor).
TARGET_PERSON_ID = 'MND-2'
# 2. Paste the exact path to the distorted image you want to test.
#    This can be any image on your Colab drive or a manually uploaded file.
EXTERNAL_DISTORTED_PATH = "/content/data_extracted/distorted/Short_distortion_final/MND-10__foggy.jpg"
# -------------------------------------

if TARGET_PERSON_ID not in gallery_embeddings:
    print(f"‚ùå ERROR: Anchor ID {TARGET_PERSON_ID} not found in the gallery. Check the ID.")
else:
    # 1. Get the Clean Reference Image Path and Data
    ref_path = reference_images_by_person[TARGET_PERSON_ID][0]
    ref_img = load_and_preprocess_image(ref_path)

    # 2. Get the Query Image Data (from the external path)
    query_path = EXTERNAL_DISTORTED_PATH
    query_img = load_and_preprocess_image(query_path)

    if query_img is None:
        print(f"‚ùå ERROR: Could not load image from path: {query_path}. Check file existence and format.")
    else:
        # 3. Generate embedding for the QUERY image
        emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

        # --- 4. Identification (Compare against ALL people) ---

        min_dist_match = float('inf')
        min_dist_wrong = float('inf')
        closest_wrong_id = "N/A"

        for person_id, emb_ref in gallery_embeddings.items():
            distance = np.linalg.norm(emb_ref - emb_query)

            if person_id == TARGET_PERSON_ID:
                min_dist_match = distance # Distance to the correct person
            elif distance < min_dist_wrong:
                min_dist_wrong = distance # Distance to the closest incorrect person
                closest_wrong_id = person_id

        # Determine the final verdict
        verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
        verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

        # --- 5. Plotting the Final Output ---

        fig, axes = plt.subplots(1, 2, figsize=(12, 6))

        fig.suptitle(f'DEPLOYMENT TEST: ID {TARGET_PERSON_ID} vs. External Query ({verdict})',
                     fontsize=14, color=verdict_color, fontweight='bold')

        # Image 1: Clean Reference
        axes[0].imshow(ref_img)
        axes[0].set_title(f'1. CORRECT REFERENCE: {TARGET_PERSON_ID}', fontsize=12)
        axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
        axes[0].axis('off')

        # Image 2: Distorted Query Image (The Input)
        axes[1].imshow(query_img)
        axes[1].set_title(f'2. EXTERNAL QUERY (Input)', fontsize=12)
        axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
        axes[1].axis('off')

        plt.tight_layout(rect=[0, 0.03, 1, 0.9])
        plt.show()

        print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
        print(f"Query Image Path used: {query_path}")
        print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
        print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
        print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (For Deployment Verification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace pretrained model."""
    try:
        tf.keras.backend.clear_session()
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # skip if face not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {os.path.basename(image_path)}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Reference and Query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display the Gender Detection Results ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (Deployment Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference Image\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender == query_gender:
        print(f"‚úÖ Gender Match Verified: Both are {ref_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Gender Mismatch Detected: {ref_gender.upper()} vs {query_gender.upper()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- Configuration (Final Settings) ---
VERIFICATION_THRESHOLD = 0.6

# --- STEP 1: DEFINE YOUR TEST CASE ---
# 1. Set the ID of the person the clean image belongs to (The Anchor).
TARGET_PERSON_ID = 'MND-1'
# 2. Paste the exact path to the distorted image you want to test.
#    This can be any image on your Colab drive or a manually uploaded file.
EXTERNAL_DISTORTED_PATH = "/content/data_refined/DISTORTED_IMG/MND-10__blurry.jpg"
# -------------------------------------

if TARGET_PERSON_ID not in gallery_embeddings:
    print(f"‚ùå ERROR: Anchor ID {TARGET_PERSON_ID} not found in the gallery. Check the ID.")
else:
    # 1. Get the Clean Reference Image Path and Data
    ref_path = reference_images_by_person[TARGET_PERSON_ID][0]
    ref_img = load_and_preprocess_image(ref_path)

    # 2. Get the Query Image Data (from the external path)
    query_path = EXTERNAL_DISTORTED_PATH
    query_img = load_and_preprocess_image(query_path)

    if query_img is None:
        print(f"‚ùå ERROR: Could not load image from path: {query_path}. Check file existence and format.")
    else:
        # 3. Generate embedding for the QUERY image
        emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

        # --- 4. Identification (Compare against ALL people) ---

        min_dist_match = float('inf')
        min_dist_wrong = float('inf')
        closest_wrong_id = "N/A"

        for person_id, emb_ref in gallery_embeddings.items():
            distance = np.linalg.norm(emb_ref - emb_query)

            if person_id == TARGET_PERSON_ID:
                min_dist_match = distance # Distance to the correct person
            elif distance < min_dist_wrong:
                min_dist_wrong = distance # Distance to the closest incorrect person
                closest_wrong_id = person_id

        # Determine the final verdict
        verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
        verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

        # --- 5. Plotting the Final Output ---

        fig, axes = plt.subplots(1, 2, figsize=(12, 6))

        fig.suptitle(f'DEPLOYMENT TEST: ID {TARGET_PERSON_ID} vs. External Query ({verdict})',
                     fontsize=14, color=verdict_color, fontweight='bold')

        # Image 1: Clean Reference
        axes[0].imshow(ref_img)
        axes[0].set_title(f'1. CORRECT REFERENCE: {TARGET_PERSON_ID}', fontsize=12)
        axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
        axes[0].axis('off')

        # Image 2: Distorted Query Image (The Input)
        axes[1].imshow(query_img)
        axes[1].set_title(f'2. EXTERNAL QUERY (Input)', fontsize=12)
        axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
        axes[1].axis('off')

        plt.tight_layout(rect=[0, 0.03, 1, 0.9])
        plt.show()

        print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
        print(f"Query Image Path used: {query_path}")
        print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
        print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
        print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (For Deployment Verification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace pretrained model."""
    try:
        tf.keras.backend.clear_session()
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # skip if face not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {os.path.basename(image_path)}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Reference and Query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display the Gender Detection Results ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (Deployment Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference Image\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender == query_gender:
        print(f"‚úÖ Gender Match Verified: Both are {ref_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Gender Mismatch Detected: {ref_gender.upper()} vs {query_gender.upper()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- Configuration (Final Settings) ---
VERIFICATION_THRESHOLD = 0.6

# --- STEP 1: DEFINE YOUR TEST CASE ---
# 1. Set the ID of the person the clean image belongs to (The Anchor).
TARGET_PERSON_ID = 'TAN-2'
# 2. Paste the exact path to the distorted image you want to test.
#    This can be any image on your Colab drive or a manually uploaded file.
EXTERNAL_DISTORTED_PATH = "/content/data_refined/DISTORTED_IMG/TAN-4__rainy.jpg"
# -------------------------------------

if TARGET_PERSON_ID not in gallery_embeddings:
    print(f"‚ùå ERROR: Anchor ID {TARGET_PERSON_ID} not found in the gallery. Check the ID.")
else:
    # 1. Get the Clean Reference Image Path and Data
    ref_path = reference_images_by_person[TARGET_PERSON_ID][0]
    ref_img = load_and_preprocess_image(ref_path)

    # 2. Get the Query Image Data (from the external path)
    query_path = EXTERNAL_DISTORTED_PATH
    query_img = load_and_preprocess_image(query_path)

    if query_img is None:
        print(f"‚ùå ERROR: Could not load image from path: {query_path}. Check file existence and format.")
    else:
        # 3. Generate embedding for the QUERY image
        emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

        # --- 4. Identification (Compare against ALL people) ---

        min_dist_match = float('inf')
        min_dist_wrong = float('inf')
        closest_wrong_id = "N/A"

        for person_id, emb_ref in gallery_embeddings.items():
            distance = np.linalg.norm(emb_ref - emb_query)

            if person_id == TARGET_PERSON_ID:
                min_dist_match = distance # Distance to the correct person
            elif distance < min_dist_wrong:
                min_dist_wrong = distance # Distance to the closest incorrect person
                closest_wrong_id = person_id

        # Determine the final verdict
        verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
        verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

        # --- 5. Plotting the Final Output ---

        fig, axes = plt.subplots(1, 2, figsize=(12, 6))

        fig.suptitle(f'DEPLOYMENT TEST: ID {TARGET_PERSON_ID} vs. External Query ({verdict})',
                     fontsize=14, color=verdict_color, fontweight='bold')

        # Image 1: Clean Reference
        axes[0].imshow(ref_img)
        axes[0].set_title(f'1. CORRECT REFERENCE: {TARGET_PERSON_ID}', fontsize=12)
        axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
        axes[0].axis('off')

        # Image 2: Distorted Query Image (The Input)
        axes[1].imshow(query_img)
        axes[1].set_title(f'2. EXTERNAL QUERY (Input)', fontsize=12)
        axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
        axes[1].axis('off')

        plt.tight_layout(rect=[0, 0.03, 1, 0.9])
        plt.show()

        print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
        print(f"Query Image Path used: {query_path}")
        print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
        print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
        print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (For Deployment Verification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace pretrained model."""
    try:
        tf.keras.backend.clear_session()
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # skip if face not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {os.path.basename(image_path)}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Reference and Query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display the Gender Detection Results ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (Deployment Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference Image\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender == query_gender:
        print(f"‚úÖ Gender Match Verified: Both are {ref_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Gender Mismatch Detected: {ref_gender.upper()} vs {query_gender.upper()}")

# ============================================================
# üß† GENDER DETECTION SECTION (For Deployment Verification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace pretrained model."""
    try:
        tf.keras.backend.clear_session()
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # skip if face not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {os.path.basename(image_path)}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Reference and Query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display the Gender Detection Results ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (Deployment Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference Image\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender == query_gender:
        print(f"‚úÖ Gender Match Verified: Both are {ref_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Gender Mismatch Detected: {ref_gender.upper()} vs {query_gender.upper()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- Configuration (Final Settings) ---
VERIFICATION_THRESHOLD = 0.6

# --- STEP 1: DEFINE YOUR TEST CASE ---
# 1. Set the ID of the person the clean image belongs to (The Anchor).
TARGET_PERSON_ID = 'TSM-2'
# 2. Paste the exact path to the distorted image you want to test.
#    This can be any image on your Colab drive or a manually uploaded file.
EXTERNAL_DISTORTED_PATH = "/content/data_extracted/distorted/Short_distortion_final/TSM-3__rainy.jpg"
# -------------------------------------

if TARGET_PERSON_ID not in gallery_embeddings:
    print(f"‚ùå ERROR: Anchor ID {TARGET_PERSON_ID} not found in the gallery. Check the ID.")
else:
    # 1. Get the Clean Reference Image Path and Data
    ref_path = reference_images_by_person[TARGET_PERSON_ID][0]
    ref_img = load_and_preprocess_image(ref_path)

    # 2. Get the Query Image Data (from the external path)
    query_path = EXTERNAL_DISTORTED_PATH
    query_img = load_and_preprocess_image(query_path)

    if query_img is None:
        print(f"‚ùå ERROR: Could not load image from path: {query_path}. Check file existence and format.")
    else:
        # 3. Generate embedding for the QUERY image
        emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

        # --- 4. Identification (Compare against ALL people) ---

        min_dist_match = float('inf')
        min_dist_wrong = float('inf')
        closest_wrong_id = "N/A"

        for person_id, emb_ref in gallery_embeddings.items():
            distance = np.linalg.norm(emb_ref - emb_query)

            if person_id == TARGET_PERSON_ID:
                min_dist_match = distance # Distance to the correct person
            elif distance < min_dist_wrong:
                min_dist_wrong = distance # Distance to the closest incorrect person
                closest_wrong_id = person_id

        # Determine the final verdict
        verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
        verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

        # --- 5. Plotting the Final Output ---

        fig, axes = plt.subplots(1, 2, figsize=(12, 6))

        fig.suptitle(f'DEPLOYMENT TEST: ID {TARGET_PERSON_ID} vs. External Query ({verdict})',
                     fontsize=14, color=verdict_color, fontweight='bold')

        # Image 1: Clean Reference
        axes[0].imshow(ref_img)
        axes[0].set_title(f'1. CORRECT REFERENCE: {TARGET_PERSON_ID}', fontsize=12)
        axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
        axes[0].axis('off')

        # Image 2: Distorted Query Image (The Input)
        axes[1].imshow(query_img)
        axes[1].set_title(f'2. EXTERNAL QUERY (Input)', fontsize=12)
        axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
        axes[1].axis('off')

        plt.tight_layout(rect=[0, 0.03, 1, 0.9])
        plt.show()

        print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
        print(f"Query Image Path used: {query_path}")
        print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
        print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
        print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (For Deployment Verification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace pretrained model."""
    try:
        tf.keras.backend.clear_session()
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # skip if face not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {os.path.basename(image_path)}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Reference and Query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display the Gender Detection Results ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (Deployment Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference Image\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender == query_gender:
        print(f"‚úÖ Gender Match Verified: Both are {ref_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Gender Mismatch Detected: {ref_gender.upper()} vs {query_gender.upper()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- Configuration (Final Settings) ---
VERIFICATION_THRESHOLD = 0.6

# --- STEP 1: DEFINE YOUR TEST CASE ---
# 1. Set the ID of the person the clean image belongs to (The Anchor).
TARGET_PERSON_ID = 'MND-2'
# 2. Paste the exact path to the distorted image you want to test.
#    This can be any image on your Colab drive or a manually uploaded file.
EXTERNAL_DISTORTED_PATH = "/content/data_refined/DISTORTED_IMG/MND-8__rainy.jpg"
# -------------------------------------

if TARGET_PERSON_ID not in gallery_embeddings:
    print(f"‚ùå ERROR: Anchor ID {TARGET_PERSON_ID} not found in the gallery. Check the ID.")
else:
    # 1. Get the Clean Reference Image Path and Data
    ref_path = reference_images_by_person[TARGET_PERSON_ID][0]
    ref_img = load_and_preprocess_image(ref_path)

    # 2. Get the Query Image Data (from the external path)
    query_path = EXTERNAL_DISTORTED_PATH
    query_img = load_and_preprocess_image(query_path)

    if query_img is None:
        print(f"‚ùå ERROR: Could not load image from path: {query_path}. Check file existence and format.")
    else:
        # 3. Generate embedding for the QUERY image
        emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

        # --- 4. Identification (Compare against ALL people) ---

        min_dist_match = float('inf')
        min_dist_wrong = float('inf')
        closest_wrong_id = "N/A"

        for person_id, emb_ref in gallery_embeddings.items():
            distance = np.linalg.norm(emb_ref - emb_query)

            if person_id == TARGET_PERSON_ID:
                min_dist_match = distance # Distance to the correct person
            elif distance < min_dist_wrong:
                min_dist_wrong = distance # Distance to the closest incorrect person
                closest_wrong_id = person_id

        # Determine the final verdict
        verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
        verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

        # --- 5. Plotting the Final Output ---

        fig, axes = plt.subplots(1, 2, figsize=(12, 6))

        fig.suptitle(f'DEPLOYMENT TEST: ID {TARGET_PERSON_ID} vs. External Query ({verdict})',
                     fontsize=14, color=verdict_color, fontweight='bold')

        # Image 1: Clean Reference
        axes[0].imshow(ref_img)
        axes[0].set_title(f'1. CORRECT REFERENCE: {TARGET_PERSON_ID}', fontsize=12)
        axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
        axes[0].axis('off')

        # Image 2: Distorted Query Image (The Input)
        axes[1].imshow(query_img)
        axes[1].set_title(f'2. EXTERNAL QUERY (Input)', fontsize=12)
        axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
        axes[1].axis('off')

        plt.tight_layout(rect=[0, 0.03, 1, 0.9])
        plt.show()

        print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
        print(f"Query Image Path used: {query_path}")
        print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
        print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
        print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (For Deployment Verification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace pretrained model."""
    try:
        tf.keras.backend.clear_session()
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # skip if face not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {os.path.basename(image_path)}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Reference and Query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display the Gender Detection Results ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (Deployment Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference Image\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender == query_gender:
        print(f"‚úÖ Gender Match Verified: Both are {ref_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Gender Mismatch Detected: {ref_gender.upper()} vs {query_gender.upper()}")

import matplotlib.pyplot as plt
import numpy as np
import random
import os
import tensorflow as tf
from tensorflow import keras

# --- Configuration (Final Settings) ---
VERIFICATION_THRESHOLD = 0.6

# --- STEP 1: DEFINE YOUR TEST CASE ---
# 1. Set the ID of the person the clean image belongs to (The Anchor).
TARGET_PERSON_ID = 'TSM-2'
# 2. Paste the exact path to the distorted image you want to test.
#    This can be any image on your Colab drive or a m/content/data_extracted/distorted/Short_distortion_final/TSM-8__blurry.jpganually uploaded file.
EXTERNAL_DISTORTED_PATH = "/content/data_extracted/distorted/Short_distortion_final/TSM-2__blurry.jpg"
# -------------------------------------

if TARGET_PERSON_ID not in gallery_embeddings:
    print(f"‚ùå ERROR: Anchor ID {TARGET_PERSON_ID} not found in the gallery. Check the ID.")
else:
    # 1. Get the Clean Reference Image Path and Data
    ref_path = reference_images_by_person[TARGET_PERSON_ID][0]
    ref_img = load_and_preprocess_image(ref_path)

    # 2. Get the Query Image Data (from the external path)
    query_path = EXTERNAL_DISTORTED_PATH
    query_img = load_and_preprocess_image(query_path)

    if query_img is None:
        print(f"‚ùå ERROR: Could not load image from path: {query_path}. Check file existence and format.")
    else:
        # 3. Generate embedding for the QUERY image
        emb_query = final_extractor_clean.predict(np.expand_dims(query_img, axis=0), verbose=0)[0]

        # --- 4. Identification (Compare against ALL people) ---

        min_dist_match = float('inf')
        min_dist_wrong = float('inf')
        closest_wrong_id = "N/A"

        for person_id, emb_ref in gallery_embeddings.items():
            distance = np.linalg.norm(emb_ref - emb_query)

            if person_id == TARGET_PERSON_ID:
                min_dist_match = distance # Distance to the correct person
            elif distance < min_dist_wrong:
                min_dist_wrong = distance # Distance to the closest incorrect person
                closest_wrong_id = person_id

        # Determine the final verdict
        verdict = "Same Person (VERIFIED)" if min_dist_match < VERIFICATION_THRESHOLD else "Different Person (FAILED)"
        verdict_color = 'green' if min_dist_match < VERIFICATION_THRESHOLD else 'red'

        # --- 5. Plotting the Final Output ---

        fig, axes = plt.subplots(1, 2, figsize=(12, 6))

        fig.suptitle(f'DEPLOYMENT TEST: ID {TARGET_PERSON_ID} vs. External Query ({verdict})',
                     fontsize=14, color=verdict_color, fontweight='bold')

        # Image 1: Clean Reference
        axes[0].imshow(ref_img)
        axes[0].set_title(f'1. CORRECT REFERENCE: {TARGET_PERSON_ID}', fontsize=12)
        axes[0].set_xlabel(f'D to Query: {min_dist_match:.4f} (MATCH)', color='green', fontsize=12)
        axes[0].axis('off')

        # Image 2: Distorted Query Image (The Input)
        axes[1].imshow(query_img)
        axes[1].set_title(f'2. EXTERNAL QUERY (Input)', fontsize=12)
        axes[1].set_xlabel(f'Closest Wrong ID: {closest_wrong_id} (D={min_dist_wrong:.4f})', color='red', fontsize=12)
        axes[1].axis('off')

        plt.tight_layout(rect=[0, 0.03, 1, 0.9])
        plt.show()

        print(f"\n--- IDENTIFICATION CONSOLE OUTPUT ---")
        print(f"Query Image Path used: {query_path}")
        print(f"Correct Match Distance (D_match): {min_dist_match:.4f}")
        print(f"Closest Incorrect Match (D_wrong): {min_dist_wrong:.4f} (ID: {closest_wrong_id})")
        print(f"Final Verdict: {verdict}")

# ============================================================
# üß† GENDER DETECTION SECTION (For Deployment Verification)
# ============================================================
!pip install deepface --quiet

from deepface import DeepFace
import cv2

def detect_gender(image_path):
    """Detect gender using DeepFace pretrained model."""
    try:
        tf.keras.backend.clear_session()
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['gender'],
            enforce_detection=False  # skip if face not perfectly detected
        )
        gender = result[0]['dominant_gender']
        confidence = result[0]['gender'][gender]
    except Exception as e:
        print(f"‚ö†Ô∏è Gender detection failed for {os.path.basename(image_path)}: {e}")
        gender = "Unknown"
        confidence = 0.0
    return gender, confidence


# --- Run Gender Detection for Reference and Query ---
ref_gender, ref_conf = detect_gender(ref_path)
query_gender, query_conf = detect_gender(query_path)


# --- Display the Gender Detection Results ---
ref_img_display = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
query_img_display = cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle("Gender Detection Results (Deployment Test)", fontsize=16, fontweight='bold')

axes[0].imshow(ref_img_display)
axes[0].set_title(f"Reference Image\nGender: {ref_gender.capitalize()} ({ref_conf:.1f}%)", fontsize=12)
axes[0].axis("off")

axes[1].imshow(query_img_display)
axes[1].set_title(f"Query Image\nGender: {query_gender.capitalize()} ({query_conf:.1f}%)", fontsize=12)
axes[1].axis("off")

plt.tight_layout(rect=[0, 0.03, 1, 0.9])
plt.show()


# --- Print Gender Summary ---
print("\n--- üß© GENDER DETECTION SUMMARY ---")
print(f"‚Ä¢ Reference ({os.path.basename(ref_path)}): {ref_gender} ({ref_conf:.1f}%)")
print(f"‚Ä¢ Query ({os.path.basename(query_path)}): {query_gender} ({query_conf:.1f}%)")

if ref_gender != "Unknown" and query_gender != "Unknown":
    if ref_gender == query_gender:
        print(f"‚úÖ Gender Match Verified: Both are {ref_gender.upper()}")
    else:
        print(f"‚ö†Ô∏è Gender Mismatch Detected: {ref_gender.upper()} vs {query_gender.upper()}")

import cv2
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from IPython.display import display, Image as IPImage
# NOTE: Assuming all functions (get_webcam_frame_manual, etc.) are available.

# --- SET VERIFICATION TARGET HERE ---
TARGET_PERSON_ID = 'TSM-10' # <--- CHANGE THIS to the ID you want to verify against!
VERIFICATION_THRESHOLD = 0.8
# ------------------------------------

if TARGET_PERSON_ID not in gallery_embeddings:
    print(f"‚ùå ERROR: Target ID {TARGET_PERSON_ID} not found in the gallery. Check the ID.")
else:
    # 1. LOAD ASSETS
    ANCHOR_EMBEDDING = gallery_embeddings[TARGET_PERSON_ID]

    try:
        ref_path = reference_images_by_person[TARGET_PERSON_ID][0]
        # Load ref_img in RGB format for matplotlib display
        ref_img_rgb = cv2.cvtColor(cv2.imread(ref_path), cv2.COLOR_BGR2RGB)
    except:
        ref_img_rgb = np.zeros((128, 128, 3), dtype=np.uint8)
        print("Warning: Could not load reference image for display.")

    # --- 2. EXECUTE MANUAL CAPTURE LOOP ---
    print(f"\n--- STARTING 1:1 VERIFICATION FOR {TARGET_PERSON_ID} ---")
    print("Click 'Capture', then wait for processing.")

    try:
        frame = get_webcam_frame_manual()
        print("‚úÖ Image captured. Processing...")

        # 3. Detect Face
        cropped_face, bbox = detect_and_crop_face(frame)

        if cropped_face is not None:
            # 4. Process and Generate Query Embedding
            processed_face = cv2.resize(cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB), (TARGET_SIZE, TARGET_SIZE))
            processed_face = processed_face.astype('float32') / 255.0
            emb_query = final_extractor_clean.predict(np.expand_dims(processed_face, axis=0), verbose=0)[0]

            # 5. Calculate 1:1 DISTANCE
            distance = np.linalg.norm(ANCHOR_EMBEDDING - emb_query)
            verdict = "MATCH" if distance < VERIFICATION_THRESHOLD else "REJECT"

            # 6. Draw Results onto the CAPTURED FRAME
            (x, y, w, h) = bbox
            color_bgr = (0, 255, 0) if verdict == "MATCH" else (0, 0, 255) # BGR color

            cv2.rectangle(frame, (x, y), (x + w, y + h), color_bgr, 2)
            label = f"{verdict} | D={distance:.2f}"
            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color_bgr, 2)

            # Convert the final result frame to RGB for plotting
            result_frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # --- 7. FINAL VISUALIZATION (Plotting Both Images) ---
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))

            # Subplot 1: Reference Image
            axes[0].imshow(ref_img_rgb)
            axes[0].set_title(f'1. STAGING: {TARGET_PERSON_ID}', fontsize=14)
            axes[0].set_xlabel('STORED ANCHOR IMAGE', fontsize=12)
            axes[0].axis('off')

            # Subplot 2: Captured and Verified Result
            axes[1].imshow(result_frame_rgb)
            axes[1].set_title(f'2. VERIFICATION RESULT: {verdict}', fontsize=14, color='green' if verdict=='MATCH' else 'red')
            axes[1].set_xlabel(f'Final Distance: {distance:.4f} (Threshold: {VERIFICATION_THRESHOLD})', fontsize=12)
            axes[1].axis('off')

            plt.tight_layout()
            plt.show()

            print("\n--- FINAL VERDICT ---")
            print(f"VERIFICATION STATUS: {verdict}")
            print(f"FINAL DISTANCE: {distance:.4f} (Threshold: {VERIFICATION_THRESHOLD})")

        else:
            print("‚ùå Face not detected in the captured image.")

    except Exception as e:
        print(f"‚ùå An error occurred: {e}")

import os
import numpy as np
import cv2
import sys

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Face embedding function (replace with your model)
# -------------------------------------------------
def get_embedding(image_path, model):
    """
    Dummy embedding function.
    Replace with your actual FaceNet/ResNet/Ensemble pipeline.
    """
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # pretend embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print("\n--- STARTING 1:1 VERIFICATION ---")
    print("Click 'Capture' and wait for processing...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        print(f"Similarity score: {sim:.4f}")
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå An error occurred: {e}")


# -------------------------------------------------
# 5. Example usage
# -------------------------------------------------
if __name__ == "__main__":
    reference_image = "/content/data_extracted/ref/short_references_final/TAN-6.jpg"  # <-- path to your stored reference
    verify_identity(reference_image)

import os
import numpy as np
import cv2
import sys

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Face embedding function (replace with your model)
# -------------------------------------------------
def get_embedding(image_path, model):
    """
    Dummy embedding function.
    Replace with your actual FaceNet/ResNet/Ensemble pipeline.
    """
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # pretend embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print("\n--- STARTING 1:1 VERIFICATION ---")
    print("Click 'Capture' and wait for processing...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        print(f"Similarity score: {sim:.4f}")
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå An error occurred: {e}")


# -------------------------------------------------
# 5. Example usage
# -------------------------------------------------
if __name__ == "__main__":
    reference_image = "/content/data_extracted/ref/short_references_final/YSP-8.jpg"  # <-- path to your stored reference
    verify_identity(reference_image)

import os
import numpy as np
import cv2
import sys

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Face embedding function (replace with your model)
# -------------------------------------------------
def get_embedding(image_path, model):
    """
    Dummy embedding function.
    Replace with your actual FaceNet/ResNet/Ensemble pipeline.
    """
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # pretend embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print("\n--- STARTING 1:1 VERIFICATION ---")
    print("Click 'Capture' and wait for processing...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        print(f"Similarity score: {sim:.4f}")
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå An error occurred: {e}")


# -------------------------------------------------
# 5. Example usage
# -------------------------------------------------
if __name__ == "__main__":
    reference_image = "/content/data_extracted/ref/short_references_final/YSP-8.jpg"  # <-- path to your stored reference
    verify_identity(reference_image)

import cv2
import matplotlib.pyplot as plt

def draw_face_box(img_path, face_box=None, color=(0,255,0)):
    """Draw bounding box if provided, else return plain image."""
    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)
    if face_box:  # (x, y, w, h)
        x, y, w, h = face_box
        cv2.rectangle(img, (x, y), (x+w, y+h), color, 3)
    return img


def show_verification_layout(ref_path, live_path, match=True, ref_box=None, live_box=None, subject_id="TSM-10"):
    # Load images
    ref_img = draw_face_box(ref_path, ref_box, (0,255,0))
    live_img = draw_face_box(live_path, live_box, (0,255,0) if match else (255,0,0))

    # Plot layout
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    axes[0].imshow(ref_img)
    axes[0].set_title(f"1. STAGING: {subject_id}", fontsize=14, weight="bold")
    axes[0].axis("off")

    axes[1].imshow(live_img)
    result_text = "MATCH" if match else "NO MATCH"
    result_color = "green" if match else "red"
    axes[1].set_title(f"2. VERIFICATION RESULT: {result_text}",
                      fontsize=14, weight="bold", color=result_color)
    axes[1].axis("off")

    plt.tight_layout()
    plt.show()

    # Final verdict
    print("\n--- FINAL VERDICT ---")
    if match:
        print(f"‚úÖ Verification SUCCESS: Subject {subject_id} confirmed.")
    else:
        print(f"‚ùå Verification FAILED: Subject {subject_id} rejected.")

import os
import numpy as np
import cv2
import sys
import matplotlib.pyplot as plt

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Dummy embedding function (replace with your real model)
# -------------------------------------------------
def get_embedding(image_path, model=None):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # dummy embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print(f"\n--- STARTING 1:1 VERIFICATION ---")
    print(f"Reference: {os.path.basename(reference_path)}")
    print("Click 'Capture' and wait...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        # Visualization
        ref_img = cv2.cvtColor(cv2.imread(reference_path), cv2.COLOR_BGR2RGB)
        live_img = cv2.cvtColor(cv2.imread(live_path), cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(8,4))
        plt.subplot(1,2,1)
        plt.imshow(ref_img)
        plt.title("Reference Image")
        plt.axis("off")

        plt.subplot(1,2,2)
        plt.imshow(live_img)
        plt.title(f"Live Image\nSimilarity={sim:.2f}")
        plt.axis("off")
        plt.show()

        # Result
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå Error: {e}")


# -------------------------------------------------
# 5. Map Dataset & Run
# -------------------------------------------------
def list_references(dataset_path):
    """List available reference images from dataset"""
    refs = []
    for root, _, files in os.walk(dataset_path):
        for f in files:
            if f.lower().endswith((".jpg", ".png", ".jpeg")):
                refs.append(os.path.join(root, f))
    return refs


if __name__ == "__main__":
    dataset_path = "/content/data_extracted/ref/short_references_final/"
    references = list_references(dataset_path)

    print("\nAvailable References:")
    for i, ref in enumerate(references):
        print(f"{i}: {ref}")

    choice = int(input("\nSelect reference index: "))
    reference_image = references[choice]

    verify_identity(reference_image)

import cv2
import numpy as np
import matplotlib.pyplot as plt

print("\n--- GENDER DETECTION SETUP ---")
male_path = input("üìÅ Enter path of a male example image: ").strip()
female_path = input("üìÅ Enter path of a female example image: ").strip()

# Load and preprocess
def load_gray(path):
    img = cv2.imread(path)
    if img is None:
        raise Exception(f"‚ùå Cannot read image: {path}")
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    gray = cv2.resize(gray, (160, 160))
    return gray

male_img = load_gray(male_path)
female_img = load_gray(female_path)

# Compute reference brightness (simple learning)
male_mean = np.mean(male_img)
female_mean = np.mean(female_img)
threshold = (male_mean + female_mean) / 2
print(f"\n‚úÖ Model trained: mean male={male_mean:.2f}, female={female_mean:.2f}, threshold={threshold:.2f}")

# Now ask user which image to check
test_path = input("\nüì∑ Enter image path to detect gender: ").strip()
test_img = load_gray(test_path)
mean_intensity = np.mean(test_img)
pred_gender = "Male" if mean_intensity < threshold else "Female"

# Show result
plt.imshow(test_img, cmap='gray')
plt.title(f"Predicted Gender: {pred_gender}", fontsize=14, color='blue')
plt.axis("off")
plt.show()

print(f"üß© Predicted Gender for {os.path.basename(test_path)}: {pred_gender}")

import os
import numpy as np
import cv2
import sys
import matplotlib.pyplot as plt

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Dummy embedding function (replace with your real model)
# -------------------------------------------------
def get_embedding(image_path, model=None):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # dummy embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print(f"\n--- STARTING 1:1 VERIFICATION ---")
    print(f"Reference: {os.path.basename(reference_path)}")
    print("Click 'Capture' and wait...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        # Visualization
        ref_img = cv2.cvtColor(cv2.imread(reference_path), cv2.COLOR_BGR2RGB)
        live_img = cv2.cvtColor(cv2.imread(live_path), cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(8,4))
        plt.subplot(1,2,1)
        plt.imshow(ref_img)
        plt.title("Reference Image")
        plt.axis("off")

        plt.subplot(1,2,2)
        plt.imshow(live_img)
        plt.title(f"Live Image\nSimilarity={sim:.2f}")
        plt.axis("off")
        plt.show()

        # Result
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå Error: {e}")


# -------------------------------------------------
# 5. Map Dataset & Run
# -------------------------------------------------
def list_references(dataset_path):
    """List available reference images from dataset"""
    refs = []
    for root, _, files in os.walk(dataset_path):
        for f in files:
            if f.lower().endswith((".jpg", ".png", ".jpeg")):
                refs.append(os.path.join(root, f))
    return refs


if __name__ == "__main__":
    dataset_path = "/content/data_extracted/ref/short_references_final/"
    references = list_references(dataset_path)

    print("\nAvailable References:")
    for i, ref in enumerate(references):
        print(f"{i}: {ref}")

    choice = int(input("\nSelect reference index: "))
    reference_image = references[choice]

    verify_identity(reference_image)

import os
import numpy as np
import cv2
import sys
import matplotlib.pyplot as plt

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Dummy embedding function (replace with your real model)
# -------------------------------------------------
def get_embedding(image_path, model=None):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # dummy embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print(f"\n--- STARTING 1:1 VERIFICATION ---")
    print(f"Reference: {os.path.basename(reference_path)}")
    print("Click 'Capture' and wait...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        # Visualization
        ref_img = cv2.cvtColor(cv2.imread(reference_path), cv2.COLOR_BGR2RGB)
        live_img = cv2.cvtColor(cv2.imread(live_path), cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(8,4))
        plt.subplot(1,2,1)
        plt.imshow(ref_img)
        plt.title("Reference Image")
        plt.axis("off")

        plt.subplot(1,2,2)
        plt.imshow(live_img)
        plt.title(f"Live Image\nSimilarity={sim:.2f}")
        plt.axis("off")
        plt.show()

        # Result
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå Error: {e}")


# -------------------------------------------------
# 5. Map Dataset & Run
# -------------------------------------------------
def list_references(dataset_path):
    """List available reference images from dataset"""
    refs = []
    for root, _, files in os.walk(dataset_path):
        for f in files:
            if f.lower().endswith((".jpg", ".png", ".jpeg")):
                refs.append(os.path.join(root, f))
    return refs


if __name__ == "__main__":
    dataset_path = "/content/data_extracted/ref/short_references_final/"
    references = list_references(dataset_path)

    print("\nAvailable References:")
    for i, ref in enumerate(references):
        print(f"{i}: {ref}")

    choice = int(input("\nSelect reference index: "))
    reference_image = references[choice]

    verify_identity(reference_image)

import os
import numpy as np
import cv2
import sys
import matplotlib.pyplot as plt

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Dummy embedding function (replace with your real model)
# -------------------------------------------------
def get_embedding(image_path, model=None):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # dummy embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print(f"\n--- STARTING 1:1 VERIFICATION ---")
    print(f"Reference: {os.path.basename(reference_path)}")
    print("Click 'Capture' and wait...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        # Visualization
        ref_img = cv2.cvtColor(cv2.imread(reference_path), cv2.COLOR_BGR2RGB)
        live_img = cv2.cvtColor(cv2.imread(live_path), cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(8,4))
        plt.subplot(1,2,1)
        plt.imshow(ref_img)
        plt.title("Reference Image")
        plt.axis("off")

        plt.subplot(1,2,2)
        plt.imshow(live_img)
        plt.title(f"Live Image\nSimilarity={sim:.2f}")
        plt.axis("off")
        plt.show()

        # Result
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå Error: {e}")


# -------------------------------------------------
# 5. Map Dataset & Run
# -------------------------------------------------
def list_references(dataset_path):
    """List available reference images from dataset"""
    refs = []
    for root, _, files in os.walk(dataset_path):
        for f in files:
            if f.lower().endswith((".jpg", ".png", ".jpeg")):
                refs.append(os.path.join(root, f))
    return refs


if __name__ == "__main__":
    dataset_path = "/content/data_extracted/ref/short_references_final/"
    references = list_references(dataset_path)

    print("\nAvailable References:")
    for i, ref in enumerate(references):
        print(f"{i}: {ref}")

    choice = int(input("\nSelect reference index: "))
    reference_image = references[choice]

    verify_identity(reference_image)

import os
import numpy as np
import cv2
import sys
import matplotlib.pyplot as plt

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Dummy embedding function (replace with your real model)
# -------------------------------------------------
def get_embedding(image_path, model=None):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # dummy embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print(f"\n--- STARTING 1:1 VERIFICATION ---")
    print(f"Reference: {os.path.basename(reference_path)}")
    print("Click 'Capture' and wait...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        # Visualization
        ref_img = cv2.cvtColor(cv2.imread(reference_path), cv2.COLOR_BGR2RGB)
        live_img = cv2.cvtColor(cv2.imread(live_path), cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(8,4))
        plt.subplot(1,2,1)
        plt.imshow(ref_img)
        plt.title("Reference Image")
        plt.axis("off")

        plt.subplot(1,2,2)
        plt.imshow(live_img)
        plt.title(f"Live Image\nSimilarity={sim:.2f}")
        plt.axis("off")
        plt.show()

        # Result
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå Error: {e}")


# -------------------------------------------------
# 5. Map Dataset & Run
# -------------------------------------------------
def list_references(dataset_path):
    """List available reference images from dataset"""
    refs = []
    for root, _, files in os.walk(dataset_path):
        for f in files:
            if f.lower().endswith((".jpg", ".png", ".jpeg")):
                refs.append(os.path.join(root, f))
    return refs


if __name__ == "__main__":
    dataset_path = "/content/data_extracted/ref/short_references_final/"
    references = list_references(dataset_path)

    print("\nAvailable References:")
    for i, ref in enumerate(references):
        print(f"{i}: {ref}")

    choice = int(input("\nSelect reference index: "))
    reference_image = references[choice]

    verify_identity(reference_image)

import os
import numpy as np
import cv2
import sys
import matplotlib.pyplot as plt

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Dummy embedding function (replace with your real model)
# -------------------------------------------------
def get_embedding(image_path, model=None):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # dummy embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print(f"\n--- STARTING 1:1 VERIFICATION ---")
    print(f"Reference: {os.path.basename(reference_path)}")
    print("Click 'Capture' and wait...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        # Visualization
        ref_img = cv2.cvtColor(cv2.imread(reference_path), cv2.COLOR_BGR2RGB)
        live_img = cv2.cvtColor(cv2.imread(live_path), cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(8,4))
        plt.subplot(1,2,1)
        plt.imshow(ref_img)
        plt.title("Reference Image")
        plt.axis("off")

        plt.subplot(1,2,2)
        plt.imshow(live_img)
        plt.title(f"Live Image\nSimilarity={sim:.2f}")
        plt.axis("off")
        plt.show()

        # Result
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå Error: {e}")


# -------------------------------------------------
# 5. Map Dataset & Run
# -------------------------------------------------
def list_references(dataset_path):
    """List available reference images from dataset"""
    refs = []
    for root, _, files in os.walk(dataset_path):
        for f in files:
            if f.lower().endswith((".jpg", ".png", ".jpeg")):
                refs.append(os.path.join(root, f))
    return refs


if __name__ == "__main__":
    dataset_path = "/content/data_extracted/ref/short_references_final/"
    references = list_references(dataset_path)

    print("\nAvailable References:")
    for i, ref in enumerate(references):
        print(f"{i}: {ref}")

    choice = int(input("\nSelect reference index: "))
    reference_image = references[choice]

    verify_identity(reference_image)

import os
import numpy as np
import cv2
import sys
import matplotlib.pyplot as plt

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Dummy embedding function (replace with your real model)
# -------------------------------------------------
def get_embedding(image_path, model=None):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # dummy embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print(f"\n--- STARTING 1:1 VERIFICATION ---")
    print(f"Reference: {os.path.basename(reference_path)}")
    print("Click 'Capture' and wait...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        # Visualization
        ref_img = cv2.cvtColor(cv2.imread(reference_path), cv2.COLOR_BGR2RGB)
        live_img = cv2.cvtColor(cv2.imread(live_path), cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(8,4))
        plt.subplot(1,2,1)
        plt.imshow(ref_img)
        plt.title("Reference Image")
        plt.axis("off")

        plt.subplot(1,2,2)
        plt.imshow(live_img)
        plt.title(f"Live Image\nSimilarity={sim:.2f}")
        plt.axis("off")
        plt.show()

        # Result
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå Error: {e}")


# -------------------------------------------------
# 5. Map Dataset & Run
# -------------------------------------------------
def list_references(dataset_path):
    """List available reference images from dataset"""
    refs = []
    for root, _, files in os.walk(dataset_path):
        for f in files:
            if f.lower().endswith((".jpg", ".png", ".jpeg")):
                refs.append(os.path.join(root, f))
    return refs


if __name__ == "__main__":
    dataset_path = "/content/data_extracted/ref/short_references_final/"
    references = list_references(dataset_path)

    print("\nAvailable References:")
    for i, ref in enumerate(references):
        print(f"{i}: {ref}")

    choice = int(input("\nSelect reference index: "))
    reference_image = references[choice]

    verify_identity(reference_image)

import os
import numpy as np
import cv2
import sys
import matplotlib.pyplot as plt

# -------------------------------------------------
# 1. Detect environment (Colab vs Local)
# -------------------------------------------------
def in_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False


# -------------------------------------------------
# 2. Capture image (Colab vs Local)
# -------------------------------------------------
if in_colab():
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    from base64 import b64decode

    def capture_image(filename='photo.jpg', quality=0.8):
        js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
        display(js)
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename

else:
    def capture_image(filename='photo.jpg'):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise Exception("‚ùå Could not open webcam")
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise Exception("‚ùå Capture failed: no frame received")
        cv2.imwrite(filename, frame)
        return filename


# -------------------------------------------------
# 3. Dummy embedding function (replace with your real model)
# -------------------------------------------------
def get_embedding(image_path, model=None):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (160, 160))
    img = img.astype("float32") / 255.0
    return img.flatten()[:128]  # dummy embedding


# -------------------------------------------------
# 4. Verification logic
# -------------------------------------------------
def verify_identity(reference_path, model=None, threshold=0.7):
    print(f"\n--- STARTING 1:1 VERIFICATION ---")
    print(f"Reference: {os.path.basename(reference_path)}")
    print("Click 'Capture' and wait...\n")

    try:
        # Capture live image
        live_path = capture_image("live_photo.jpg")

        # Extract embeddings
        ref_emb = get_embedding(reference_path, model)
        live_emb = get_embedding(live_path, model)

        # Cosine similarity
        sim = np.dot(ref_emb, live_emb) / (np.linalg.norm(ref_emb) * np.linalg.norm(live_emb))

        # Visualization
        ref_img = cv2.cvtColor(cv2.imread(reference_path), cv2.COLOR_BGR2RGB)
        live_img = cv2.cvtColor(cv2.imread(live_path), cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(8,4))
        plt.subplot(1,2,1)
        plt.imshow(ref_img)
        plt.title("Reference Image")
        plt.axis("off")

        plt.subplot(1,2,2)
        plt.imshow(live_img)
        plt.title(f"Live Image\nSimilarity={sim:.2f}")
        plt.axis("off")
        plt.show()

        # Result
        if sim >= threshold:
            print("‚úÖ Verification SUCCESS: Same person")
        else:
            print("‚ùå Verification FAILED: Different person")

    except Exception as e:
        print(f"‚ùå Error: {e}")


# -------------------------------------------------
# 5. Map Dataset & Run
# -------------------------------------------------
def list_references(dataset_path):
    """List available reference images from dataset"""
    refs = []
    for root, _, files in os.walk(dataset_path):
        for f in files:
            if f.lower().endswith((".jpg", ".png", ".jpeg")):
                refs.append(os.path.join(root, f))
    return refs


if __name__ == "__main__":
    dataset_path = "/content/data_extracted/ref/short_references_final/"
    references = list_references(dataset_path)

    print("\nAvailable References:")
    for i, ref in enumerate(references):
        print(f"{i}: {ref}")

    choice = int(input("\nSelect reference index: "))
    reference_image = references[choice]

    verify_identity(reference_image)

# Save and verify
model.save("face_embedding_model_CLEAN.h5", include_optimizer=False)

import os
print("Model size (MB):", os.path.getsize("face_embedding_model_CLEAN.h5")/1024/1024)

from google.colab import files
files.download("face_embedding_model_CLEAN.h5")